{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All files are available from [here](https://drive.google.com/drive/folders/1feVwyTfeGU4MBfE0d1xYm290esazWv7P?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeR2g7VFJur8"
      },
      "outputs": [],
      "source": [
        "# !cp -r /content/drive/MyDrive/HighLevel_Tasks/Data /content/\n",
        "!cp /content/drive/MyDrive/HighLevel_Tasks/analysis_utils.py /content/\n",
        "!cp /content/drive/MyDrive/HighLevel_Tasks/correlation_analysis.py /content/\n",
        "!cp /content/drive/MyDrive/HighLevel_Tasks/get_results.py /content/\n",
        "!cp /content/drive/MyDrive/HighLevel_Tasks/mbert_finetune_xnli.sh /content/\n",
        "!cp /content/drive/MyDrive/HighLevel_Tasks/mbert_finetune_xnli_eval_only.sh /content/\n",
        "!cp /content/drive/MyDrive/HighLevel_Tasks/mbert_finetune_xnli_retrain.sh /content/\n",
        "!cp /content/drive/MyDrive/HighLevel_Tasks/meta_learner.py /content/\n",
        "!cp /content/drive/MyDrive/HighLevel_Tasks/meta_learner_ffs.py /content/\n",
        "!cp /content/drive/MyDrive/HighLevel_Tasks/reserve_xnli_test_examples.py /content/\n",
        "!cp /content/drive/MyDrive/HighLevel_Tasks/run_xnli.py /content/\n",
        "!cp /content/drive/MyDrive/HighLevel_Tasks/xnli_processor.py /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR02ddiYJyGD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers==2.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlYvcRtPJ09K",
        "outputId": "88792190-8e72-4831-baf3-2c1b09ec964b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VFDQW4uJ5d-"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lucQRcAIKGQ0"
      },
      "source": [
        "## Few-shot-after:\n",
        "Fine-tune with the source language and, then, continue fine-tuning with few-shots from the target language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ruh_UxmKG9R"
      },
      "source": [
        "Phase One: Fine-tune with English as the source language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQNEiMamKLXY",
        "outputId": "13b9e58a-a199-4761-bc85-97ba08cefd71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 100% 12272/12272 [37:43<00:00,  5.42it/s]\n",
            "Epoch: 100% 2/2 [1:15:17<00:00, 2258.74s/it]\n",
            "10/08/2024 14:39:19 - INFO - __main__ -    global_step = 24544, average loss = 0.49184705568550363\n",
            "10/08/2024 14:39:19 - INFO - __main__ -   Saving model checkpoint to /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0\n",
            "10/08/2024 14:39:19 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/config.json\n",
            "10/08/2024 14:39:21 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 14:39:21 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/config.json\n",
            "10/08/2024 14:39:21 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 14:39:21 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 14:39:25 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 14:39:25 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/vocab.txt\n",
            "10/08/2024 14:39:25 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 14:39:25 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 14:39:25 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 14:39:25 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 14:39:25 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/vocab.txt\n",
            "10/08/2024 14:39:25 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 14:39:25 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 14:39:25 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 14:39:25 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0']\n",
            "10/08/2024 14:39:25 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/config.json\n",
            "10/08/2024 14:39:25 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 14:39:25 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 14:39:30 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   guid: test-10021\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 21404 10726 12426 10221 13132 90998 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   label: contradiction (id = 0)\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   guid: test-10022\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 10338 10380 10221 29789 117 11064 12979 54068 12426 10151 105111 10221 13132 10304 55492 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   guid: test-10023\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 51732 17547 10290 81754 10171 144 106979 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   guid: test-10024\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 43209 10338 10726 10347 91754 117 11064 12979 10726 10128 29991 30325 10329 10268 53709 10338 10151 14001 17033 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   guid: test-10025\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 21023 11569 10140 97611 61139 117 11064 12979 10211 26523 11565 11704 13384 84209 16003 10118 29991 10221 11906 44145 10338 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:39:32 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 14:39:35 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_test_bert-base-multilingual-cased_128_xnli_de\n",
            "10/08/2024 14:39:36 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/08/2024 14:39:36 - INFO - __main__ -     Num examples = 5010\n",
            "10/08/2024 14:39:36 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 627/627 [00:10<00:00, 60.40it/s]\n",
            "10/08/2024 14:39:46 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/08/2024 14:39:46 - INFO - __main__ -     acc = 0.7087824351297405\n"
          ]
        }
      ],
      "source": [
        "# k = 0 (only source)\n",
        "!python run_xnli.py --model_type bert --model_name_or_path bert-base-multilingual-cased --language de --train_language en --do_train --do_eval --data_dir \"/content/drive/MyDrive/HighLevel_Tasks/Data/\" --per_gpu_train_batch_size 32 --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 128 --output_dir \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0\" --save_steps -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K53B5g2hJ_3-"
      },
      "source": [
        "Phase Two: Continue fine-tuning with few-shots (k = 10) from German as the target language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAq_NMQEKQSL",
        "outputId": "be29debd-f29f-4da9-a3af-57a3021c3177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-10-08 14:59:40.224169: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-08 14:59:40.245213: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-08 14:59:40.251726: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-08 14:59:41.310544: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/08/2024 14:59:43 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/08/2024 14:59:43 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/config.json\n",
            "10/08/2024 14:59:43 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 14:59:43 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 14:59:43 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/vocab.txt\n",
            "10/08/2024 14:59:43 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 14:59:43 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 14:59:43 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 14:59:43 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 14:59:48 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='/content/drive/MyDrive/HighLevel_Tasks/Data/', model_type='bert', model_name_or_path='/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0', language='de', train_language='de', num_examples=10, sampling_strategy='k_first', output_dir='/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0', config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=True, do_eval_dev=False, do_eval=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=32, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, warmup_steps=0, logging_steps=500, save_steps=-1, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=True, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), task_name='xnli', output_mode='classification')\n",
            "10/08/2024 14:59:48 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   guid: dev-6535\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   input_ids: 101 51732 17684 10681 10968 80123 117 15826 10304 75031 119 102 51732 67603 59953 35435 10150 48773 117 10128 10372 15826 11744 93148 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   guid: dev-5282\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   input_ids: 101 10599 10118 28099 57281 117 10128 33963 107282 10112 20284 10298 55592 98170 78564 102 78564 10298 10290 28099 21062 117 10140 33963 88076 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   guid: dev-6651\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   input_ids: 101 10236 28373 21779 70332 29917 48977 10130 70332 29917 11280 15967 11230 20428 10112 11930 64515 12111 11216 10130 94496 11088 11270 10106 44803 117 41266 117 26077 117 18776 10130 10211 23816 119 102 13830 64515 12111 11216 49661 29923 83550 99256 30518 36133 145 41559 10112 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   guid: dev-7383\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   input_ids: 101 155 67043 13755 10492 55260 10237 17048 12563 60167 117 11566 74224 10112 47003 10304 119 102 10236 30731 10118 74224 10112 10298 46503 85671 10136 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   guid: dev-7328\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   input_ids: 101 10912 10298 23930 23973 117 10128 70722 10136 12145 11157 12808 10106 10599 17989 10136 35145 10441 82876 11909 117 16082 110749 10466 10251 59464 107606 117 144 40100 10130 12045 60969 10112 10441 10268 30186 11227 28883 10221 10290 11044 41771 10615 119 102 11045 70832 10562 10106 10599 17989 10136 35145 12382 10817 10128 45708 47411 10714 10128 61828 56946 10716 10118 110749 10466 10251 59464 107606 117 144 40100 10130 65998 28883 11044 41771 10136 55194 92394 11216 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 14:59:49 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 14:59:49 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_train_xnli_en_3e-5_2.0_128_xnli_de\n",
            "10/08/2024 14:59:50 - INFO - __main__ -   ***** Running training *****\n",
            "10/08/2024 14:59:50 - INFO - __main__ -     Num examples = 10\n",
            "10/08/2024 14:59:50 - INFO - __main__ -     Num Epochs = 2\n",
            "10/08/2024 14:59:50 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
            "10/08/2024 14:59:50 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "10/08/2024 14:59:50 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "10/08/2024 14:59:50 - INFO - __main__ -     Total optimization steps = 2\n",
            "10/08/2024 14:59:50 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
            "10/08/2024 14:59:50 - INFO - __main__ -     Continuing training from epoch 0\n",
            "10/08/2024 14:59:50 - INFO - __main__ -     Continuing training from global step 0\n",
            "10/08/2024 14:59:50 - INFO - __main__ -     Will skip the first 0 steps in the first epoch\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/1 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1581.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration: 100% 1/1 [00:00<00:00,  1.17it/s]\n",
            "Epoch:  50% 1/2 [00:00<00:00,  1.17it/s]\n",
            "Iteration: 100% 1/1 [00:00<00:00, 11.13it/s]\n",
            "Epoch: 100% 2/2 [00:00<00:00,  2.12it/s]\n",
            "10/08/2024 14:59:51 - INFO - __main__ -    global_step = 2, average loss = 0.2856065109372139\n",
            "10/08/2024 14:59:51 - INFO - __main__ -   Saving model checkpoint to /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0\n",
            "10/08/2024 14:59:51 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0/config.json\n",
            "10/08/2024 14:59:53 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 14:59:53 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0/config.json\n",
            "10/08/2024 14:59:53 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 14:59:53 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 14:59:57 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 14:59:57 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0/vocab.txt\n",
            "10/08/2024 14:59:57 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 14:59:57 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 14:59:57 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 14:59:58 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 14:59:58 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0/vocab.txt\n",
            "10/08/2024 14:59:58 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 14:59:58 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 14:59:58 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 14:59:58 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0']\n",
            "10/08/2024 14:59:58 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0/config.json\n",
            "10/08/2024 14:59:58 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 14:59:58 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 15:00:03 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   guid: test-10021\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 21404 10726 12426 10221 13132 90998 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   label: contradiction (id = 0)\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   guid: test-10022\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 10338 10380 10221 29789 117 11064 12979 54068 12426 10151 105111 10221 13132 10304 55492 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   guid: test-10023\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 51732 17547 10290 81754 10171 144 106979 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   guid: test-10024\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 43209 10338 10726 10347 91754 117 11064 12979 10726 10128 29991 30325 10329 10268 53709 10338 10151 14001 17033 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   guid: test-10025\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 21023 11569 10140 97611 61139 117 11064 12979 10211 26523 11565 11704 13384 84209 16003 10118 29991 10221 11906 44145 10338 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:00:04 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 15:00:07 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_test_xnli_en_3e-5_2.0_128_xnli_de\n",
            "10/08/2024 15:00:08 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/08/2024 15:00:08 - INFO - __main__ -     Num examples = 5010\n",
            "10/08/2024 15:00:08 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 627/627 [00:10<00:00, 60.19it/s]\n",
            "10/08/2024 15:00:18 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/08/2024 15:00:18 - INFO - __main__ -     acc = 0.7001996007984032\n"
          ]
        }
      ],
      "source": [
        "# k = 10\n",
        "!python run_xnli.py --model_type bert --model_name_or_path \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0\" --language de --train_language de --do_eval --do_train --overwrite_cache --num_examples 10 --sampling_strategy \"k_first\" --data_dir \"/content/drive/MyDrive/HighLevel_Tasks/Data/\" --seed 2 --per_gpu_train_batch_size 32 --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 128 --output_dir \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k10_3e-5_2.0\" --save_steps -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOkFLBFdqEWe",
        "outputId": "e684f22f-35d8-448a-f565-e3f44cc6d1b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-10-08 15:05:57.405289: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-08 15:05:57.426463: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-08 15:05:57.432908: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-08 15:05:58.498141: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/08/2024 15:06:00 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/08/2024 15:06:00 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/config.json\n",
            "10/08/2024 15:06:00 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 15:06:00 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 15:06:00 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/vocab.txt\n",
            "10/08/2024 15:06:00 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 15:06:00 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 15:06:00 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 15:06:00 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 15:06:05 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='/content/drive/MyDrive/HighLevel_Tasks/Data/', model_type='bert', model_name_or_path='/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0', language='de', train_language='de', num_examples=50, sampling_strategy='k_first', output_dir='/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0', config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=True, do_eval_dev=False, do_eval=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=32, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, warmup_steps=0, logging_steps=500, save_steps=-1, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=True, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), task_name='xnli', output_mode='classification')\n",
            "10/08/2024 15:06:05 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   guid: dev-6535\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   input_ids: 101 51732 17684 10681 10968 80123 117 15826 10304 75031 119 102 51732 67603 59953 35435 10150 48773 117 10128 10372 15826 11744 93148 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   guid: dev-5282\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   input_ids: 101 10599 10118 28099 57281 117 10128 33963 107282 10112 20284 10298 55592 98170 78564 102 78564 10298 10290 28099 21062 117 10140 33963 88076 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   guid: dev-6651\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   input_ids: 101 10236 28373 21779 70332 29917 48977 10130 70332 29917 11280 15967 11230 20428 10112 11930 64515 12111 11216 10130 94496 11088 11270 10106 44803 117 41266 117 26077 117 18776 10130 10211 23816 119 102 13830 64515 12111 11216 49661 29923 83550 99256 30518 36133 145 41559 10112 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   guid: dev-7383\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   input_ids: 101 155 67043 13755 10492 55260 10237 17048 12563 60167 117 11566 74224 10112 47003 10304 119 102 10236 30731 10118 74224 10112 10298 46503 85671 10136 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   guid: dev-7328\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   input_ids: 101 10912 10298 23930 23973 117 10128 70722 10136 12145 11157 12808 10106 10599 17989 10136 35145 10441 82876 11909 117 16082 110749 10466 10251 59464 107606 117 144 40100 10130 12045 60969 10112 10441 10268 30186 11227 28883 10221 10290 11044 41771 10615 119 102 11045 70832 10562 10106 10599 17989 10136 35145 12382 10817 10128 45708 47411 10714 10128 61828 56946 10716 10118 110749 10466 10251 59464 107606 117 144 40100 10130 65998 28883 11044 41771 10136 55194 92394 11216 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:06 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 15:06:06 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_train_xnli_en_3e-5_2.0_128_xnli_de\n",
            "10/08/2024 15:06:06 - INFO - __main__ -   ***** Running training *****\n",
            "10/08/2024 15:06:06 - INFO - __main__ -     Num examples = 50\n",
            "10/08/2024 15:06:06 - INFO - __main__ -     Num Epochs = 2\n",
            "10/08/2024 15:06:06 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
            "10/08/2024 15:06:06 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "10/08/2024 15:06:06 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "10/08/2024 15:06:06 - INFO - __main__ -     Total optimization steps = 4\n",
            "10/08/2024 15:06:06 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
            "10/08/2024 15:06:06 - INFO - __main__ -     Continuing training from epoch 0\n",
            "10/08/2024 15:06:06 - INFO - __main__ -     Continuing training from global step 0\n",
            "10/08/2024 15:06:06 - INFO - __main__ -     Will skip the first 0 steps in the first epoch\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/2 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1581.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:  50% 1/2 [00:01<00:01,  1.04s/it]\u001b[A\n",
            "Iteration: 100% 2/2 [00:01<00:00,  1.74it/s]\n",
            "Epoch:  50% 1/2 [00:01<00:01,  1.15s/it]\n",
            "Iteration:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:  50% 1/2 [00:00<00:00,  5.45it/s]\u001b[A\n",
            "Iteration: 100% 2/2 [00:00<00:00,  6.79it/s]\n",
            "Epoch: 100% 2/2 [00:01<00:00,  1.38it/s]\n",
            "10/08/2024 15:06:08 - INFO - __main__ -    global_step = 4, average loss = 0.5041275396943092\n",
            "10/08/2024 15:06:08 - INFO - __main__ -   Saving model checkpoint to /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0\n",
            "10/08/2024 15:06:08 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0/config.json\n",
            "10/08/2024 15:06:10 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 15:06:10 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0/config.json\n",
            "10/08/2024 15:06:10 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 15:06:10 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 15:06:14 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 15:06:14 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0/vocab.txt\n",
            "10/08/2024 15:06:14 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 15:06:14 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 15:06:14 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 15:06:15 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 15:06:15 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0/vocab.txt\n",
            "10/08/2024 15:06:15 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 15:06:15 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 15:06:15 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 15:06:15 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0']\n",
            "10/08/2024 15:06:15 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0/config.json\n",
            "10/08/2024 15:06:15 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 15:06:15 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 15:06:19 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   guid: test-10021\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 21404 10726 12426 10221 13132 90998 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   label: contradiction (id = 0)\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   guid: test-10022\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 10338 10380 10221 29789 117 11064 12979 54068 12426 10151 105111 10221 13132 10304 55492 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   guid: test-10023\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 51732 17547 10290 81754 10171 144 106979 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   guid: test-10024\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 43209 10338 10726 10347 91754 117 11064 12979 10726 10128 29991 30325 10329 10268 53709 10338 10151 14001 17033 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   guid: test-10025\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 21023 11569 10140 97611 61139 117 11064 12979 10211 26523 11565 11704 13384 84209 16003 10118 29991 10221 11906 44145 10338 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:06:21 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 15:06:23 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_test_xnli_en_3e-5_2.0_128_xnli_de\n",
            "10/08/2024 15:06:24 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/08/2024 15:06:24 - INFO - __main__ -     Num examples = 5010\n",
            "10/08/2024 15:06:24 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 627/627 [00:10<00:00, 59.96it/s]\n",
            "10/08/2024 15:06:35 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/08/2024 15:06:35 - INFO - __main__ -     acc = 0.7153692614770459\n"
          ]
        }
      ],
      "source": [
        "# k = 50\n",
        "!python run_xnli.py --model_type bert --model_name_or_path \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0\" --language de --train_language de --do_eval --do_train --overwrite_cache --num_examples 50 --sampling_strategy \"k_first\" --data_dir \"/content/drive/MyDrive/HighLevel_Tasks/Data/\" --seed 2 --per_gpu_train_batch_size 32 --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 128 --output_dir \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k50_3e-5_2.0\" --save_steps -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYLKkY8dp1-h",
        "outputId": "15d0df80-eab5-4eb8-eb33-bd9c44e51cda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-10-08 15:03:10.524832: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-08 15:03:10.546358: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-08 15:03:10.552803: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-08 15:03:11.596657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/08/2024 15:03:13 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/08/2024 15:03:13 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/config.json\n",
            "10/08/2024 15:03:13 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 15:03:13 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 15:03:13 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/vocab.txt\n",
            "10/08/2024 15:03:13 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 15:03:13 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 15:03:13 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 15:03:13 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 15:03:18 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='/content/drive/MyDrive/HighLevel_Tasks/Data/', model_type='bert', model_name_or_path='/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0', language='de', train_language='de', num_examples=100, sampling_strategy='k_first', output_dir='/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0', config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=True, do_eval_dev=False, do_eval=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=32, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, warmup_steps=0, logging_steps=500, save_steps=-1, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=True, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), task_name='xnli', output_mode='classification')\n",
            "10/08/2024 15:03:18 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   guid: dev-6535\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   input_ids: 101 51732 17684 10681 10968 80123 117 15826 10304 75031 119 102 51732 67603 59953 35435 10150 48773 117 10128 10372 15826 11744 93148 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   guid: dev-5282\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   input_ids: 101 10599 10118 28099 57281 117 10128 33963 107282 10112 20284 10298 55592 98170 78564 102 78564 10298 10290 28099 21062 117 10140 33963 88076 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   guid: dev-6651\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   input_ids: 101 10236 28373 21779 70332 29917 48977 10130 70332 29917 11280 15967 11230 20428 10112 11930 64515 12111 11216 10130 94496 11088 11270 10106 44803 117 41266 117 26077 117 18776 10130 10211 23816 119 102 13830 64515 12111 11216 49661 29923 83550 99256 30518 36133 145 41559 10112 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   guid: dev-7383\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   input_ids: 101 155 67043 13755 10492 55260 10237 17048 12563 60167 117 11566 74224 10112 47003 10304 119 102 10236 30731 10118 74224 10112 10298 46503 85671 10136 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   guid: dev-7328\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   input_ids: 101 10912 10298 23930 23973 117 10128 70722 10136 12145 11157 12808 10106 10599 17989 10136 35145 10441 82876 11909 117 16082 110749 10466 10251 59464 107606 117 144 40100 10130 12045 60969 10112 10441 10268 30186 11227 28883 10221 10290 11044 41771 10615 119 102 11045 70832 10562 10106 10599 17989 10136 35145 12382 10817 10128 45708 47411 10714 10128 61828 56946 10716 10118 110749 10466 10251 59464 107606 117 144 40100 10130 65998 28883 11044 41771 10136 55194 92394 11216 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:19 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 15:03:19 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_train_xnli_en_3e-5_2.0_128_xnli_de\n",
            "10/08/2024 15:03:20 - INFO - __main__ -   ***** Running training *****\n",
            "10/08/2024 15:03:20 - INFO - __main__ -     Num examples = 100\n",
            "10/08/2024 15:03:20 - INFO - __main__ -     Num Epochs = 2\n",
            "10/08/2024 15:03:20 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
            "10/08/2024 15:03:20 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "10/08/2024 15:03:20 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "10/08/2024 15:03:20 - INFO - __main__ -     Total optimization steps = 8\n",
            "10/08/2024 15:03:20 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
            "10/08/2024 15:03:20 - INFO - __main__ -     Continuing training from epoch 0\n",
            "10/08/2024 15:03:20 - INFO - __main__ -     Continuing training from global step 0\n",
            "10/08/2024 15:03:20 - INFO - __main__ -     Will skip the first 0 steps in the first epoch\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/4 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1581.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:  25% 1/4 [00:01<00:03,  1.02s/it]\u001b[A\n",
            "Iteration:  50% 2/4 [00:01<00:01,  1.85it/s]\u001b[A\n",
            "Iteration: 100% 4/4 [00:01<00:00,  2.70it/s]\n",
            "Epoch:  50% 1/2 [00:01<00:01,  1.48s/it]\n",
            "Iteration:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:  25% 1/4 [00:00<00:00,  5.46it/s]\u001b[A\n",
            "Iteration:  50% 2/4 [00:00<00:00,  5.46it/s]\u001b[A\n",
            "Iteration: 100% 4/4 [00:00<00:00,  6.59it/s]\n",
            "Epoch: 100% 2/2 [00:02<00:00,  1.05s/it]\n",
            "10/08/2024 15:03:22 - INFO - __main__ -    global_step = 8, average loss = 0.47242251969873905\n",
            "10/08/2024 15:03:22 - INFO - __main__ -   Saving model checkpoint to /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0\n",
            "10/08/2024 15:03:22 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0/config.json\n",
            "10/08/2024 15:03:24 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 15:03:24 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0/config.json\n",
            "10/08/2024 15:03:24 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 15:03:24 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 15:03:28 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 15:03:28 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0/vocab.txt\n",
            "10/08/2024 15:03:28 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 15:03:28 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 15:03:28 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 15:03:29 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 15:03:29 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0/vocab.txt\n",
            "10/08/2024 15:03:29 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 15:03:29 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 15:03:29 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 15:03:29 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0']\n",
            "10/08/2024 15:03:29 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0/config.json\n",
            "10/08/2024 15:03:29 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 15:03:29 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 15:03:33 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   guid: test-10021\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 21404 10726 12426 10221 13132 90998 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   label: contradiction (id = 0)\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   guid: test-10022\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 10338 10380 10221 29789 117 11064 12979 54068 12426 10151 105111 10221 13132 10304 55492 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   guid: test-10023\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 51732 17547 10290 81754 10171 144 106979 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:34 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 15:03:35 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:03:35 - INFO - transformers.data.processors.glue -   guid: test-10024\n",
            "10/08/2024 15:03:35 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 43209 10338 10726 10347 91754 117 11064 12979 10726 10128 29991 30325 10329 10268 53709 10338 10151 14001 17033 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:35 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:35 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:35 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 15:03:35 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:03:35 - INFO - transformers.data.processors.glue -   guid: test-10025\n",
            "10/08/2024 15:03:35 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 21023 11569 10140 97611 61139 117 11064 12979 10211 26523 11565 11704 13384 84209 16003 10118 29991 10221 11906 44145 10338 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:35 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:35 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:03:35 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 15:03:37 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_test_xnli_en_3e-5_2.0_128_xnli_de\n",
            "10/08/2024 15:03:38 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/08/2024 15:03:38 - INFO - __main__ -     Num examples = 5010\n",
            "10/08/2024 15:03:38 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 627/627 [00:10<00:00, 60.53it/s]\n",
            "10/08/2024 15:03:49 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/08/2024 15:03:49 - INFO - __main__ -     acc = 0.7227544910179641\n"
          ]
        }
      ],
      "source": [
        "# k = 100\n",
        "!python run_xnli.py --model_type bert --model_name_or_path \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0\" --language de --train_language de --do_eval --do_train --overwrite_cache --num_examples 100 --sampling_strategy \"k_first\" --data_dir \"/content/drive/MyDrive/HighLevel_Tasks/Data/\" --seed 2 --per_gpu_train_batch_size 32 --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 128 --output_dir \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k100_3e-5_2.0\" --save_steps -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScaSERgFp2YQ",
        "outputId": "5e3adc02-2666-4e92-8e1a-423a009872e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-10-08 15:03:52.713234: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-08 15:03:52.738894: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-08 15:03:52.746765: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-08 15:03:53.821600: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/08/2024 15:03:56 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/08/2024 15:03:56 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/config.json\n",
            "10/08/2024 15:03:56 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 15:03:56 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 15:03:56 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/vocab.txt\n",
            "10/08/2024 15:03:56 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 15:03:56 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 15:03:56 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 15:03:56 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 15:04:01 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='/content/drive/MyDrive/HighLevel_Tasks/Data/', model_type='bert', model_name_or_path='/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0', language='de', train_language='de', num_examples=1000, sampling_strategy='k_first', output_dir='/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0', config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=True, do_eval_dev=False, do_eval=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=32, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, warmup_steps=0, logging_steps=500, save_steps=-1, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=True, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), task_name='xnli', output_mode='classification')\n",
            "10/08/2024 15:04:01 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   guid: dev-6535\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   input_ids: 101 51732 17684 10681 10968 80123 117 15826 10304 75031 119 102 51732 67603 59953 35435 10150 48773 117 10128 10372 15826 11744 93148 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   guid: dev-5282\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   input_ids: 101 10599 10118 28099 57281 117 10128 33963 107282 10112 20284 10298 55592 98170 78564 102 78564 10298 10290 28099 21062 117 10140 33963 88076 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   guid: dev-6651\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   input_ids: 101 10236 28373 21779 70332 29917 48977 10130 70332 29917 11280 15967 11230 20428 10112 11930 64515 12111 11216 10130 94496 11088 11270 10106 44803 117 41266 117 26077 117 18776 10130 10211 23816 119 102 13830 64515 12111 11216 49661 29923 83550 99256 30518 36133 145 41559 10112 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   guid: dev-7383\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   input_ids: 101 155 67043 13755 10492 55260 10237 17048 12563 60167 117 11566 74224 10112 47003 10304 119 102 10236 30731 10118 74224 10112 10298 46503 85671 10136 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   guid: dev-7328\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   input_ids: 101 10912 10298 23930 23973 117 10128 70722 10136 12145 11157 12808 10106 10599 17989 10136 35145 10441 82876 11909 117 16082 110749 10466 10251 59464 107606 117 144 40100 10130 12045 60969 10112 10441 10268 30186 11227 28883 10221 10290 11044 41771 10615 119 102 11045 70832 10562 10106 10599 17989 10136 35145 12382 10817 10128 45708 47411 10714 10128 61828 56946 10716 10118 110749 10466 10251 59464 107606 117 144 40100 10130 65998 28883 11044 41771 10136 55194 92394 11216 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:01 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 15:04:02 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_train_xnli_en_3e-5_2.0_128_xnli_de\n",
            "10/08/2024 15:04:02 - INFO - __main__ -   ***** Running training *****\n",
            "10/08/2024 15:04:02 - INFO - __main__ -     Num examples = 1000\n",
            "10/08/2024 15:04:02 - INFO - __main__ -     Num Epochs = 2\n",
            "10/08/2024 15:04:02 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
            "10/08/2024 15:04:02 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "10/08/2024 15:04:02 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "10/08/2024 15:04:02 - INFO - __main__ -     Total optimization steps = 64\n",
            "10/08/2024 15:04:02 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
            "10/08/2024 15:04:02 - INFO - __main__ -     Continuing training from epoch 0\n",
            "10/08/2024 15:04:02 - INFO - __main__ -     Continuing training from global step 0\n",
            "10/08/2024 15:04:02 - INFO - __main__ -     Will skip the first 0 steps in the first epoch\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/32 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1581.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration: 100% 32/32 [00:06<00:00,  4.81it/s]\n",
            "Epoch:  50% 1/2 [00:06<00:06,  6.65s/it]\n",
            "Iteration: 100% 32/32 [00:05<00:00,  5.54it/s]\n",
            "Epoch: 100% 2/2 [00:12<00:00,  6.21s/it]\n",
            "10/08/2024 15:04:15 - INFO - __main__ -    global_step = 64, average loss = 0.5123444801429287\n",
            "10/08/2024 15:04:15 - INFO - __main__ -   Saving model checkpoint to /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0\n",
            "10/08/2024 15:04:15 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0/config.json\n",
            "10/08/2024 15:04:17 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 15:04:17 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0/config.json\n",
            "10/08/2024 15:04:17 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 15:04:17 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 15:04:21 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 15:04:21 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0/vocab.txt\n",
            "10/08/2024 15:04:21 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 15:04:21 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 15:04:21 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 15:04:22 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 15:04:22 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0/vocab.txt\n",
            "10/08/2024 15:04:22 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 15:04:22 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 15:04:22 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 15:04:22 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0']\n",
            "10/08/2024 15:04:22 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0/config.json\n",
            "10/08/2024 15:04:22 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 15:04:22 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 15:04:27 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   guid: test-10021\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 21404 10726 12426 10221 13132 90998 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   label: contradiction (id = 0)\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   guid: test-10022\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 10338 10380 10221 29789 117 11064 12979 54068 12426 10151 105111 10221 13132 10304 55492 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   guid: test-10023\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 51732 17547 10290 81754 10171 144 106979 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   guid: test-10024\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 43209 10338 10726 10347 91754 117 11064 12979 10726 10128 29991 30325 10329 10268 53709 10338 10151 14001 17033 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   guid: test-10025\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 21023 11569 10140 97611 61139 117 11064 12979 10211 26523 11565 11704 13384 84209 16003 10118 29991 10221 11906 44145 10338 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 15:04:28 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 15:04:31 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_test_xnli_en_3e-5_2.0_128_xnli_de\n",
            "10/08/2024 15:04:32 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/08/2024 15:04:32 - INFO - __main__ -     Num examples = 5010\n",
            "10/08/2024 15:04:32 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 627/627 [00:10<00:00, 60.61it/s]\n",
            "10/08/2024 15:04:42 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/08/2024 15:04:42 - INFO - __main__ -     acc = 0.7365269461077845\n"
          ]
        }
      ],
      "source": [
        "# k = 1000\n",
        "!python run_xnli.py --model_type bert --model_name_or_path \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_3e-5_2.0\" --language de --train_language de --do_eval --do_train --overwrite_cache --num_examples 1000 --sampling_strategy \"k_first\" --data_dir \"/content/drive/MyDrive/HighLevel_Tasks/Data/\" --seed 2 --per_gpu_train_batch_size 32 --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 128 --output_dir \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_en_de_k1000_3e-5_2.0\" --save_steps -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Few-shot-before"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByeP-IDCKQ5L"
      },
      "source": [
        "### Two: k = 10\n",
        "Fine-tune with few-shots from the target language and, then, continue fine-tuning with the source language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxBA8uP_KUXS"
      },
      "source": [
        "Phase One: Fine-tune with few-shots (k = 10) from German as the target language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D77g0fzQKYaz",
        "outputId": "bd80f3f1-54de-4a59-9652-cd47a975a491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-10-08 16:17:37.166125: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-08 16:17:37.187270: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-08 16:17:37.193779: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-08 16:17:38.249602: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/08/2024 16:17:40 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/08/2024 16:17:40 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /root/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
            "10/08/2024 16:17:40 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 16:17:41 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /root/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "10/08/2024 16:17:41 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin from cache at /root/.cache/torch/transformers/5b5b80054cd2c95a946a8e0ce0b93f56326dff9fbda6a6c3e02de3c91c918342.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 16:17:45 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "10/08/2024 16:17:45 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "10/08/2024 16:17:46 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='/content/drive/MyDrive/HighLevel_Tasks/Data/', model_type='bert', model_name_or_path='bert-base-multilingual-cased', language='de', train_language='de', num_examples=10, sampling_strategy='k_first', output_dir='/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0', config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=True, do_eval_dev=False, do_eval=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=32, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, warmup_steps=0, logging_steps=500, save_steps=-1, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=True, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), task_name='xnli', output_mode='classification')\n",
            "10/08/2024 16:17:46 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   guid: dev-6535\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   input_ids: 101 51732 17684 10681 10968 80123 117 15826 10304 75031 119 102 51732 67603 59953 35435 10150 48773 117 10128 10372 15826 11744 93148 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   guid: dev-5282\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   input_ids: 101 10599 10118 28099 57281 117 10128 33963 107282 10112 20284 10298 55592 98170 78564 102 78564 10298 10290 28099 21062 117 10140 33963 88076 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   guid: dev-6651\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   input_ids: 101 10236 28373 21779 70332 29917 48977 10130 70332 29917 11280 15967 11230 20428 10112 11930 64515 12111 11216 10130 94496 11088 11270 10106 44803 117 41266 117 26077 117 18776 10130 10211 23816 119 102 13830 64515 12111 11216 49661 29923 83550 99256 30518 36133 145 41559 10112 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   guid: dev-7383\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   input_ids: 101 155 67043 13755 10492 55260 10237 17048 12563 60167 117 11566 74224 10112 47003 10304 119 102 10236 30731 10118 74224 10112 10298 46503 85671 10136 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   guid: dev-7328\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   input_ids: 101 10912 10298 23930 23973 117 10128 70722 10136 12145 11157 12808 10106 10599 17989 10136 35145 10441 82876 11909 117 16082 110749 10466 10251 59464 107606 117 144 40100 10130 12045 60969 10112 10441 10268 30186 11227 28883 10221 10290 11044 41771 10615 119 102 11045 70832 10562 10106 10599 17989 10136 35145 12382 10817 10128 45708 47411 10714 10128 61828 56946 10716 10118 110749 10466 10251 59464 107606 117 144 40100 10130 65998 28883 11044 41771 10136 55194 92394 11216 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:46 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:17:46 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_train_bert-base-multilingual-cased_128_xnli_de\n",
            "10/08/2024 16:17:47 - INFO - __main__ -   ***** Running training *****\n",
            "10/08/2024 16:17:47 - INFO - __main__ -     Num examples = 10\n",
            "10/08/2024 16:17:47 - INFO - __main__ -     Num Epochs = 2\n",
            "10/08/2024 16:17:47 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
            "10/08/2024 16:17:47 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "10/08/2024 16:17:47 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "10/08/2024 16:17:47 - INFO - __main__ -     Total optimization steps = 2\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/1 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1581.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration: 100% 1/1 [00:00<00:00,  1.06it/s]\n",
            "Epoch:  50% 1/2 [00:00<00:00,  1.06it/s]\n",
            "Iteration: 100% 1/1 [00:00<00:00, 11.07it/s]\n",
            "Epoch: 100% 2/2 [00:01<00:00,  1.93it/s]\n",
            "10/08/2024 16:17:48 - INFO - __main__ -    global_step = 2, average loss = 1.0937588810920715\n",
            "10/08/2024 16:17:48 - INFO - __main__ -   Saving model checkpoint to /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0\n",
            "10/08/2024 16:17:48 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0/config.json\n",
            "10/08/2024 16:17:50 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 16:17:50 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0/config.json\n",
            "10/08/2024 16:17:50 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 16:17:50 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 16:17:54 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 16:17:54 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0/vocab.txt\n",
            "10/08/2024 16:17:54 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 16:17:54 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 16:17:54 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 16:17:55 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 16:17:55 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0/vocab.txt\n",
            "10/08/2024 16:17:55 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 16:17:55 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 16:17:55 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 16:17:55 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0']\n",
            "10/08/2024 16:17:55 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0/config.json\n",
            "10/08/2024 16:17:55 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 16:17:55 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 16:18:00 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   guid: test-10021\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 21404 10726 12426 10221 13132 90998 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   label: contradiction (id = 0)\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   guid: test-10022\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 10338 10380 10221 29789 117 11064 12979 54068 12426 10151 105111 10221 13132 10304 55492 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   guid: test-10023\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 51732 17547 10290 81754 10171 144 106979 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   guid: test-10024\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 43209 10338 10726 10347 91754 117 11064 12979 10726 10128 29991 30325 10329 10268 53709 10338 10151 14001 17033 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   guid: test-10025\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 21023 11569 10140 97611 61139 117 11064 12979 10211 26523 11565 11704 13384 84209 16003 10118 29991 10221 11906 44145 10338 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:18:01 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:18:04 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_test_bert-base-multilingual-cased_128_xnli_de\n",
            "10/08/2024 16:18:05 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/08/2024 16:18:05 - INFO - __main__ -     Num examples = 5010\n",
            "10/08/2024 16:18:05 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 627/627 [00:10<00:00, 59.73it/s]\n",
            "10/08/2024 16:18:16 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/08/2024 16:18:16 - INFO - __main__ -     acc = 0.33053892215568864\n"
          ]
        }
      ],
      "source": [
        "!python run_xnli.py --model_type bert --model_name_or_path bert-base-multilingual-cased --language de --train_language de --do_eval --do_train --overwrite_cache --overwrite_output_dir --num_examples 10 --sampling_strategy \"k_first\" --data_dir \"/content/drive/MyDrive/HighLevel_Tasks/Data/\" --seed 2 --per_gpu_train_batch_size 32 --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 128 --output_dir \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0\" --save_steps -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVEUouQvKYu7"
      },
      "source": [
        "Phase Two: Continue fine-tuning with English as the source language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EcfKrrNKb1W",
        "outputId": "787eb8a9-de92-4dc3-eed0-60ff4eb5032d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|█████████▉| 12271/12272 [37:44<00:00,  5.36it/s]\u001b[A\n",
            "Iteration: 100%|██████████| 12272/12272 [37:44<00:00,  5.42it/s]\n",
            "Epoch: 100%|██████████| 2/2 [1:15:30<00:00, 2265.24s/it]\n",
            "10/08/2024 17:48:29 - INFO - __main__ -    global_step = 24544, average loss = 0.49426848778890925\n",
            "10/08/2024 17:48:29 - INFO - __main__ -   Saving model checkpoint to /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0\n",
            "10/08/2024 17:48:29 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0/config.json\n",
            "10/08/2024 17:48:30 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 17:48:31 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0/config.json\n",
            "10/08/2024 17:48:31 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 17:48:31 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 17:48:35 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 17:48:35 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0/vocab.txt\n",
            "10/08/2024 17:48:35 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 17:48:35 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 17:48:35 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 17:48:35 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 17:48:35 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0/vocab.txt\n",
            "10/08/2024 17:48:35 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 17:48:35 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 17:48:35 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 17:48:35 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0']\n",
            "10/08/2024 17:48:35 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0/config.json\n",
            "10/08/2024 17:48:35 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 17:48:35 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 17:48:40 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   guid: test-10021\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 21404 10726 12426 10221 13132 90998 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   label: contradiction (id = 0)\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   guid: test-10022\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 10338 10380 10221 29789 117 11064 12979 54068 12426 10151 105111 10221 13132 10304 55492 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   guid: test-10023\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 51732 17547 10290 81754 10171 144 106979 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   guid: test-10024\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 43209 10338 10726 10347 91754 117 11064 12979 10726 10128 29991 30325 10329 10268 53709 10338 10151 14001 17033 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   guid: test-10025\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 21023 11569 10140 97611 61139 117 11064 12979 10211 26523 11565 11704 13384 84209 16003 10118 29991 10221 11906 44145 10338 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 17:48:41 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 17:48:44 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_test_xnli_de_k10_3e-5_2.0_128_xnli_de\n",
            "10/08/2024 17:48:45 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/08/2024 17:48:45 - INFO - __main__ -     Num examples = 5010\n",
            "10/08/2024 17:48:45 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100%|██████████| 627/627 [00:10<00:00, 60.08it/s]\n",
            "10/08/2024 17:48:55 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/08/2024 17:48:55 - INFO - __main__ -     acc = 0.7105788423153693\n"
          ]
        }
      ],
      "source": [
        "!python run_xnli.py --model_type bert --model_name_or_path \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_3e-5_2.0\" --language de --train_language en --do_train --do_eval --data_dir \"/content/drive/MyDrive/HighLevel_Tasks/Data/\" --per_gpu_train_batch_size 32 --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 128 --output_dir \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k10_en_3e-5_2.0\" --save_steps -1 2>&1 | tee /content/drive/MyDrive/HighLevel_Tasks/dek10_en.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWo0obcT39_V"
      },
      "source": [
        "### Two:  k = 50\n",
        "Fine-tune with few-shots from the target language and, then, continue fine-tuning with the source language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgdLO1UT39_W"
      },
      "source": [
        "Phase One: Fine-tune with few-shots (k = 50) from German as the target language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfkdG5Rr39_W",
        "outputId": "37c9b1a8-2409-4c4f-83c1-b7fe30e6a773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-10-08 16:14:46.233444: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-08 16:14:46.254067: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-08 16:14:46.260392: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-08 16:14:47.294728: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/08/2024 16:14:49 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/08/2024 16:14:49 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /root/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
            "10/08/2024 16:14:49 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 16:14:50 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /root/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "10/08/2024 16:14:50 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpcp1b_58t\n",
            "10/08/2024 16:15:15 - INFO - transformers.file_utils -   copying /tmp/tmpcp1b_58t to cache at /root/.cache/torch/transformers/5b5b80054cd2c95a946a8e0ce0b93f56326dff9fbda6a6c3e02de3c91c918342.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
            "10/08/2024 16:15:16 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/5b5b80054cd2c95a946a8e0ce0b93f56326dff9fbda6a6c3e02de3c91c918342.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
            "10/08/2024 16:15:16 - INFO - transformers.file_utils -   removing temp file /tmp/tmpcp1b_58t\n",
            "10/08/2024 16:15:16 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin from cache at /root/.cache/torch/transformers/5b5b80054cd2c95a946a8e0ce0b93f56326dff9fbda6a6c3e02de3c91c918342.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 16:15:21 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "10/08/2024 16:15:21 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "10/08/2024 16:15:21 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='/content/drive/MyDrive/HighLevel_Tasks/Data/', model_type='bert', model_name_or_path='bert-base-multilingual-cased', language='de', train_language='de', num_examples=50, sampling_strategy='k_first', output_dir='/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0', config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=True, do_eval_dev=False, do_eval=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=32, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, warmup_steps=0, logging_steps=500, save_steps=-1, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=True, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), task_name='xnli', output_mode='classification')\n",
            "10/08/2024 16:15:21 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   guid: dev-6535\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   input_ids: 101 51732 17684 10681 10968 80123 117 15826 10304 75031 119 102 51732 67603 59953 35435 10150 48773 117 10128 10372 15826 11744 93148 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   guid: dev-5282\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   input_ids: 101 10599 10118 28099 57281 117 10128 33963 107282 10112 20284 10298 55592 98170 78564 102 78564 10298 10290 28099 21062 117 10140 33963 88076 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   guid: dev-6651\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   input_ids: 101 10236 28373 21779 70332 29917 48977 10130 70332 29917 11280 15967 11230 20428 10112 11930 64515 12111 11216 10130 94496 11088 11270 10106 44803 117 41266 117 26077 117 18776 10130 10211 23816 119 102 13830 64515 12111 11216 49661 29923 83550 99256 30518 36133 145 41559 10112 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   guid: dev-7383\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   input_ids: 101 155 67043 13755 10492 55260 10237 17048 12563 60167 117 11566 74224 10112 47003 10304 119 102 10236 30731 10118 74224 10112 10298 46503 85671 10136 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   guid: dev-7328\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   input_ids: 101 10912 10298 23930 23973 117 10128 70722 10136 12145 11157 12808 10106 10599 17989 10136 35145 10441 82876 11909 117 16082 110749 10466 10251 59464 107606 117 144 40100 10130 12045 60969 10112 10441 10268 30186 11227 28883 10221 10290 11044 41771 10615 119 102 11045 70832 10562 10106 10599 17989 10136 35145 12382 10817 10128 45708 47411 10714 10128 61828 56946 10716 10118 110749 10466 10251 59464 107606 117 144 40100 10130 65998 28883 11044 41771 10136 55194 92394 11216 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:23 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:15:23 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_train_bert-base-multilingual-cased_128_xnli_de\n",
            "10/08/2024 16:15:24 - INFO - __main__ -   ***** Running training *****\n",
            "10/08/2024 16:15:24 - INFO - __main__ -     Num examples = 50\n",
            "10/08/2024 16:15:24 - INFO - __main__ -     Num Epochs = 2\n",
            "10/08/2024 16:15:24 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
            "10/08/2024 16:15:24 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "10/08/2024 16:15:24 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "10/08/2024 16:15:24 - INFO - __main__ -     Total optimization steps = 4\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/2 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1581.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:  50% 1/2 [00:01<00:01,  1.94s/it]\u001b[A\n",
            "Iteration: 100% 2/2 [00:02<00:00,  1.04s/it]\n",
            "Epoch:  50% 1/2 [00:02<00:02,  2.08s/it]\n",
            "Iteration:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:  50% 1/2 [00:00<00:00,  5.11it/s]\u001b[A\n",
            "Iteration: 100% 2/2 [00:00<00:00,  6.52it/s]\n",
            "Epoch: 100% 2/2 [00:02<00:00,  1.19s/it]\n",
            "10/08/2024 16:15:26 - INFO - __main__ -    global_step = 4, average loss = 1.1340069770812988\n",
            "10/08/2024 16:15:26 - INFO - __main__ -   Saving model checkpoint to /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0\n",
            "10/08/2024 16:15:26 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0/config.json\n",
            "10/08/2024 16:15:28 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 16:15:28 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0/config.json\n",
            "10/08/2024 16:15:28 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 16:15:28 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 16:15:33 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 16:15:33 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0/vocab.txt\n",
            "10/08/2024 16:15:33 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 16:15:33 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 16:15:33 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 16:15:33 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 16:15:33 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0/vocab.txt\n",
            "10/08/2024 16:15:33 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 16:15:33 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 16:15:33 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 16:15:33 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0']\n",
            "10/08/2024 16:15:33 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0/config.json\n",
            "10/08/2024 16:15:33 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 16:15:33 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 16:15:38 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   guid: test-10021\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 21404 10726 12426 10221 13132 90998 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   label: contradiction (id = 0)\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   guid: test-10022\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 10338 10380 10221 29789 117 11064 12979 54068 12426 10151 105111 10221 13132 10304 55492 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   guid: test-10023\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 51732 17547 10290 81754 10171 144 106979 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   guid: test-10024\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 43209 10338 10726 10347 91754 117 11064 12979 10726 10128 29991 30325 10329 10268 53709 10338 10151 14001 17033 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   guid: test-10025\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 21023 11569 10140 97611 61139 117 11064 12979 10211 26523 11565 11704 13384 84209 16003 10118 29991 10221 11906 44145 10338 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:15:40 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:15:43 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_test_bert-base-multilingual-cased_128_xnli_de\n",
            "10/08/2024 16:15:45 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/08/2024 16:15:45 - INFO - __main__ -     Num examples = 5010\n",
            "10/08/2024 16:15:45 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 627/627 [00:10<00:00, 60.18it/s]\n",
            "10/08/2024 16:15:55 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/08/2024 16:15:55 - INFO - __main__ -     acc = 0.330938123752495\n"
          ]
        }
      ],
      "source": [
        "!python run_xnli.py --model_type bert --model_name_or_path bert-base-multilingual-cased --language de --train_language de --do_eval --do_train --overwrite_cache --overwrite_output_dir --num_examples 50 --sampling_strategy \"k_first\" --data_dir \"/content/drive/MyDrive/HighLevel_Tasks/Data/\" --seed 2 --per_gpu_train_batch_size 32 --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 128 --output_dir \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0\" --save_steps -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxR0rTLE39_W"
      },
      "source": [
        "Phase Two: Continue fine-tuning with English as the source language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7g51jLq39_W",
        "outputId": "352b76f0-146d-4a22-a0a4-f1401a50cdd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|█████████▉| 12271/12272 [37:40<00:00,  5.43it/s]\u001b[A\n",
            "Iteration: 100%|██████████| 12272/12272 [37:40<00:00,  5.43it/s]\n",
            "Epoch: 100%|██████████| 2/2 [1:15:23<00:00, 2261.91s/it]\n",
            "10/08/2024 19:19:23 - INFO - __main__ -    global_step = 24544, average loss = 0.4926071485766784\n",
            "10/08/2024 19:19:23 - INFO - __main__ -   Saving model checkpoint to /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0\n",
            "10/08/2024 19:19:23 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0/config.json\n",
            "10/08/2024 19:19:24 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 19:19:24 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0/config.json\n",
            "10/08/2024 19:19:24 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 19:19:25 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 19:19:29 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 19:19:29 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0/vocab.txt\n",
            "10/08/2024 19:19:29 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 19:19:29 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 19:19:29 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 19:19:29 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 19:19:29 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0/vocab.txt\n",
            "10/08/2024 19:19:29 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 19:19:29 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 19:19:29 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 19:19:29 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0']\n",
            "10/08/2024 19:19:29 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0/config.json\n",
            "10/08/2024 19:19:29 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 19:19:29 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 19:19:34 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   guid: test-10021\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 21404 10726 12426 10221 13132 90998 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   label: contradiction (id = 0)\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   guid: test-10022\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 10338 10380 10221 29789 117 11064 12979 54068 12426 10151 105111 10221 13132 10304 55492 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   guid: test-10023\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 51732 17547 10290 81754 10171 144 106979 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   guid: test-10024\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 43209 10338 10726 10347 91754 117 11064 12979 10726 10128 29991 30325 10329 10268 53709 10338 10151 14001 17033 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   guid: test-10025\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 21023 11569 10140 97611 61139 117 11064 12979 10211 26523 11565 11704 13384 84209 16003 10118 29991 10221 11906 44145 10338 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 19:19:35 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 19:19:38 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_test_xnli_de_k50_3e-5_2.0_128_xnli_de\n",
            "10/08/2024 19:19:39 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/08/2024 19:19:39 - INFO - __main__ -     Num examples = 5010\n",
            "10/08/2024 19:19:39 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100%|██████████| 627/627 [00:10<00:00, 60.11it/s]\n",
            "10/08/2024 19:19:49 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/08/2024 19:19:49 - INFO - __main__ -     acc = 0.7179640718562874\n"
          ]
        }
      ],
      "source": [
        "!python run_xnli.py --model_type bert --model_name_or_path \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_3e-5_2.0\" --language de --train_language en --do_train --do_eval --data_dir \"/content/drive/MyDrive/HighLevel_Tasks/Data/\" --per_gpu_train_batch_size 32 --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 128 --output_dir \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k50_en_3e-5_2.0\" --save_steps -1 2>&1 | tee /content/drive/MyDrive/HighLevel_Tasks/dek50_en.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4UxrZaK3-fk"
      },
      "source": [
        "### Two:  k = 100\n",
        "Fine-tune with few-shots from the target language and, then, continue fine-tuning with the source language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlFhUWnG3-fk"
      },
      "source": [
        "Phase One: Fine-tune with few-shots (k = 100) from German as the target language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "960htMtC3-fk",
        "outputId": "52320e58-0cb0-403d-85a1-a81088a7f729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-10-08 16:15:59.865110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-08 16:15:59.885720: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-08 16:15:59.892018: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-08 16:16:00.962905: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/08/2024 16:16:03 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/08/2024 16:16:03 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /root/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
            "10/08/2024 16:16:03 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 16:16:03 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /root/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "10/08/2024 16:16:04 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin from cache at /root/.cache/torch/transformers/5b5b80054cd2c95a946a8e0ce0b93f56326dff9fbda6a6c3e02de3c91c918342.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 16:16:08 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "10/08/2024 16:16:08 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "10/08/2024 16:16:09 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='/content/drive/MyDrive/HighLevel_Tasks/Data/', model_type='bert', model_name_or_path='bert-base-multilingual-cased', language='de', train_language='de', num_examples=100, sampling_strategy='k_first', output_dir='/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0', config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=True, do_eval_dev=False, do_eval=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=32, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, warmup_steps=0, logging_steps=500, save_steps=-1, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=True, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), task_name='xnli', output_mode='classification')\n",
            "10/08/2024 16:16:09 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   guid: dev-6535\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   input_ids: 101 51732 17684 10681 10968 80123 117 15826 10304 75031 119 102 51732 67603 59953 35435 10150 48773 117 10128 10372 15826 11744 93148 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   guid: dev-5282\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   input_ids: 101 10599 10118 28099 57281 117 10128 33963 107282 10112 20284 10298 55592 98170 78564 102 78564 10298 10290 28099 21062 117 10140 33963 88076 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   guid: dev-6651\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   input_ids: 101 10236 28373 21779 70332 29917 48977 10130 70332 29917 11280 15967 11230 20428 10112 11930 64515 12111 11216 10130 94496 11088 11270 10106 44803 117 41266 117 26077 117 18776 10130 10211 23816 119 102 13830 64515 12111 11216 49661 29923 83550 99256 30518 36133 145 41559 10112 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   guid: dev-7383\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   input_ids: 101 155 67043 13755 10492 55260 10237 17048 12563 60167 117 11566 74224 10112 47003 10304 119 102 10236 30731 10118 74224 10112 10298 46503 85671 10136 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   guid: dev-7328\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   input_ids: 101 10912 10298 23930 23973 117 10128 70722 10136 12145 11157 12808 10106 10599 17989 10136 35145 10441 82876 11909 117 16082 110749 10466 10251 59464 107606 117 144 40100 10130 12045 60969 10112 10441 10268 30186 11227 28883 10221 10290 11044 41771 10615 119 102 11045 70832 10562 10106 10599 17989 10136 35145 12382 10817 10128 45708 47411 10714 10128 61828 56946 10716 10118 110749 10466 10251 59464 107606 117 144 40100 10130 65998 28883 11044 41771 10136 55194 92394 11216 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:09 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:16:09 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_train_bert-base-multilingual-cased_128_xnli_de\n",
            "10/08/2024 16:16:10 - INFO - __main__ -   ***** Running training *****\n",
            "10/08/2024 16:16:10 - INFO - __main__ -     Num examples = 100\n",
            "10/08/2024 16:16:10 - INFO - __main__ -     Num Epochs = 2\n",
            "10/08/2024 16:16:10 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
            "10/08/2024 16:16:10 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "10/08/2024 16:16:10 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "10/08/2024 16:16:10 - INFO - __main__ -     Total optimization steps = 8\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/4 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1581.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:  25% 1/4 [00:01<00:03,  1.13s/it]\u001b[A\n",
            "Iteration:  50% 2/4 [00:01<00:01,  1.71it/s]\u001b[A\n",
            "Iteration: 100% 4/4 [00:01<00:00,  2.52it/s]\n",
            "Epoch:  50% 1/2 [00:01<00:01,  1.59s/it]\n",
            "Iteration:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:  25% 1/4 [00:00<00:00,  5.41it/s]\u001b[A\n",
            "Iteration:  50% 2/4 [00:00<00:00,  5.43it/s]\u001b[A\n",
            "Iteration: 100% 4/4 [00:00<00:00,  6.61it/s]\n",
            "Epoch: 100% 2/2 [00:02<00:00,  1.10s/it]\n",
            "10/08/2024 16:16:12 - INFO - __main__ -    global_step = 8, average loss = 1.1139521151781082\n",
            "10/08/2024 16:16:12 - INFO - __main__ -   Saving model checkpoint to /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0\n",
            "10/08/2024 16:16:12 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0/config.json\n",
            "10/08/2024 16:16:14 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 16:16:14 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0/config.json\n",
            "10/08/2024 16:16:14 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 16:16:14 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 16:16:18 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 16:16:18 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0/vocab.txt\n",
            "10/08/2024 16:16:18 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 16:16:18 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 16:16:18 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 16:16:19 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 16:16:19 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0/vocab.txt\n",
            "10/08/2024 16:16:19 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 16:16:19 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 16:16:19 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 16:16:19 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0']\n",
            "10/08/2024 16:16:19 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0/config.json\n",
            "10/08/2024 16:16:19 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 16:16:19 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 16:16:24 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   guid: test-10021\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 21404 10726 12426 10221 13132 90998 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   label: contradiction (id = 0)\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   guid: test-10022\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 10338 10380 10221 29789 117 11064 12979 54068 12426 10151 105111 10221 13132 10304 55492 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   guid: test-10023\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 51732 17547 10290 81754 10171 144 106979 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   guid: test-10024\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 43209 10338 10726 10347 91754 117 11064 12979 10726 10128 29991 30325 10329 10268 53709 10338 10151 14001 17033 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   guid: test-10025\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 21023 11569 10140 97611 61139 117 11064 12979 10211 26523 11565 11704 13384 84209 16003 10118 29991 10221 11906 44145 10338 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:25 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:16:28 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_test_bert-base-multilingual-cased_128_xnli_de\n",
            "10/08/2024 16:16:28 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/08/2024 16:16:28 - INFO - __main__ -     Num examples = 5010\n",
            "10/08/2024 16:16:28 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 627/627 [00:10<00:00, 60.24it/s]\n",
            "10/08/2024 16:16:39 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/08/2024 16:16:39 - INFO - __main__ -     acc = 0.35169660678642717\n"
          ]
        }
      ],
      "source": [
        "!python run_xnli.py --model_type bert --model_name_or_path bert-base-multilingual-cased --language de --train_language de --do_eval --do_train --overwrite_cache --overwrite_output_dir --num_examples 100 --sampling_strategy \"k_first\" --data_dir \"/content/drive/MyDrive/HighLevel_Tasks/Data/\" --seed 2 --per_gpu_train_batch_size 32 --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 128 --output_dir \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0\" --save_steps -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caAt3t_J3-fl"
      },
      "source": [
        "Phase Two: Continue fine-tuning with English as the source language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2t0ETUf3-fl",
        "outputId": "f9f5b25f-d866-4ae4-9c94-d6feaad9b561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 12272/12272 [37:48<00:00,  5.41it/s]\n",
            "Epoch: 100%|██████████| 2/2 [1:15:36<00:00, 2268.06s/it]\n",
            "10/09/2024 14:57:46 - INFO - __main__ -    global_step = 24544, average loss = 0.4917128388472436\n",
            "10/09/2024 14:57:46 - INFO - __main__ -   Saving model checkpoint to /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0\n",
            "10/09/2024 14:57:46 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0/config.json\n",
            "10/09/2024 14:57:47 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0/pytorch_model.bin\n",
            "10/09/2024 14:57:48 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0/config.json\n",
            "10/09/2024 14:57:48 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/09/2024 14:57:48 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/09/2024 14:57:52 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/09/2024 14:57:52 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0/vocab.txt\n",
            "10/09/2024 14:57:52 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0/added_tokens.json\n",
            "10/09/2024 14:57:52 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0/special_tokens_map.json\n",
            "10/09/2024 14:57:52 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0/tokenizer_config.json\n",
            "10/09/2024 14:57:52 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/09/2024 14:57:52 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0/vocab.txt\n",
            "10/09/2024 14:57:52 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0/added_tokens.json\n",
            "10/09/2024 14:57:52 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0/special_tokens_map.json\n",
            "10/09/2024 14:57:52 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0/tokenizer_config.json\n",
            "10/09/2024 14:57:52 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0']\n",
            "10/09/2024 14:57:52 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0/config.json\n",
            "10/09/2024 14:57:52 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/09/2024 14:57:52 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0/pytorch_model.bin\n",
            "10/09/2024 14:57:57 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   guid: test-10021\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 21404 10726 12426 10221 13132 90998 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   label: contradiction (id = 0)\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   guid: test-10022\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 10338 10380 10221 29789 117 11064 12979 54068 12426 10151 105111 10221 13132 10304 55492 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   guid: test-10023\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 51732 17547 10290 81754 10171 144 106979 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   guid: test-10024\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 43209 10338 10726 10347 91754 117 11064 12979 10726 10128 29991 30325 10329 10268 53709 10338 10151 14001 17033 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   guid: test-10025\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 21023 11569 10140 97611 61139 117 11064 12979 10211 26523 11565 11704 13384 84209 16003 10118 29991 10221 11906 44145 10338 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 14:57:58 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/09/2024 14:58:01 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_test_xnli_de_k100_3e-5_2.0_128_xnli_de\n",
            "10/09/2024 14:58:02 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/09/2024 14:58:02 - INFO - __main__ -     Num examples = 5010\n",
            "10/09/2024 14:58:02 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100%|██████████| 627/627 [00:10<00:00, 60.11it/s]\n",
            "10/09/2024 14:58:13 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/09/2024 14:58:13 - INFO - __main__ -     acc = 0.7147704590818363\n"
          ]
        }
      ],
      "source": [
        "!python run_xnli.py --model_type bert --model_name_or_path \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_3e-5_2.0\" --language de --train_language en --do_train --do_eval --data_dir \"/content/drive/MyDrive/HighLevel_Tasks/Data/\" --per_gpu_train_batch_size 32 --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 128 --output_dir \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k100_en_3e-5_2.0\" --save_steps -1 2>&1 | tee /content/drive/MyDrive/HighLevel_Tasks/dek100_en.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_qzxgC33_Eu"
      },
      "source": [
        "### Two: k = 1000\n",
        "Fine-tune with few-shots from the target language and, then, continue fine-tuning with the source language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcUx3LKQ3_Ev"
      },
      "source": [
        "Phase One: Fine-tune with few-shots (k = 1000) from German as the target language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6uukzD63_Ev",
        "outputId": "cfbad6d9-e636-46a7-f927-a5dada495cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-10-08 16:16:42.977321: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-08 16:16:42.998497: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-08 16:16:43.004885: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-08 16:16:44.055234: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/08/2024 16:16:46 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/08/2024 16:16:46 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /root/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
            "10/08/2024 16:16:46 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 16:16:46 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /root/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "10/08/2024 16:16:47 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin from cache at /root/.cache/torch/transformers/5b5b80054cd2c95a946a8e0ce0b93f56326dff9fbda6a6c3e02de3c91c918342.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 16:16:51 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "10/08/2024 16:16:51 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "10/08/2024 16:16:52 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='/content/drive/MyDrive/HighLevel_Tasks/Data/', model_type='bert', model_name_or_path='bert-base-multilingual-cased', language='de', train_language='de', num_examples=1000, sampling_strategy='k_first', output_dir='/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0', config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=True, do_eval_dev=False, do_eval=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=32, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, warmup_steps=0, logging_steps=500, save_steps=-1, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=True, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), task_name='xnli', output_mode='classification')\n",
            "10/08/2024 16:16:52 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   guid: dev-6535\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   input_ids: 101 51732 17684 10681 10968 80123 117 15826 10304 75031 119 102 51732 67603 59953 35435 10150 48773 117 10128 10372 15826 11744 93148 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   guid: dev-5282\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   input_ids: 101 10599 10118 28099 57281 117 10128 33963 107282 10112 20284 10298 55592 98170 78564 102 78564 10298 10290 28099 21062 117 10140 33963 88076 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   guid: dev-6651\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   input_ids: 101 10236 28373 21779 70332 29917 48977 10130 70332 29917 11280 15967 11230 20428 10112 11930 64515 12111 11216 10130 94496 11088 11270 10106 44803 117 41266 117 26077 117 18776 10130 10211 23816 119 102 13830 64515 12111 11216 49661 29923 83550 99256 30518 36133 145 41559 10112 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   guid: dev-7383\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   input_ids: 101 155 67043 13755 10492 55260 10237 17048 12563 60167 117 11566 74224 10112 47003 10304 119 102 10236 30731 10118 74224 10112 10298 46503 85671 10136 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   guid: dev-7328\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   input_ids: 101 10912 10298 23930 23973 117 10128 70722 10136 12145 11157 12808 10106 10599 17989 10136 35145 10441 82876 11909 117 16082 110749 10466 10251 59464 107606 117 144 40100 10130 12045 60969 10112 10441 10268 30186 11227 28883 10221 10290 11044 41771 10615 119 102 11045 70832 10562 10106 10599 17989 10136 35145 12382 10817 10128 45708 47411 10714 10128 61828 56946 10716 10118 110749 10466 10251 59464 107606 117 144 40100 10130 65998 28883 11044 41771 10136 55194 92394 11216 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:16:52 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:16:53 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_train_bert-base-multilingual-cased_128_xnli_de\n",
            "10/08/2024 16:16:53 - INFO - __main__ -   ***** Running training *****\n",
            "10/08/2024 16:16:53 - INFO - __main__ -     Num examples = 1000\n",
            "10/08/2024 16:16:53 - INFO - __main__ -     Num Epochs = 2\n",
            "10/08/2024 16:16:53 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
            "10/08/2024 16:16:53 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "10/08/2024 16:16:53 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "10/08/2024 16:16:53 - INFO - __main__ -     Total optimization steps = 64\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/32 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1581.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration: 100% 32/32 [00:06<00:00,  4.78it/s]\n",
            "Epoch:  50% 1/2 [00:06<00:06,  6.69s/it]\n",
            "Iteration: 100% 32/32 [00:05<00:00,  5.53it/s]\n",
            "Epoch: 100% 2/2 [00:12<00:00,  6.24s/it]\n",
            "10/08/2024 16:17:06 - INFO - __main__ -    global_step = 64, average loss = 1.0848515927791595\n",
            "10/08/2024 16:17:06 - INFO - __main__ -   Saving model checkpoint to /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0\n",
            "10/08/2024 16:17:06 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0/config.json\n",
            "10/08/2024 16:17:08 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 16:17:08 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0/config.json\n",
            "10/08/2024 16:17:08 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 16:17:08 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/08/2024 16:17:12 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 16:17:12 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0/vocab.txt\n",
            "10/08/2024 16:17:12 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 16:17:12 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 16:17:12 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 16:17:13 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/08/2024 16:17:13 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0/vocab.txt\n",
            "10/08/2024 16:17:13 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0/added_tokens.json\n",
            "10/08/2024 16:17:13 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0/special_tokens_map.json\n",
            "10/08/2024 16:17:13 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0/tokenizer_config.json\n",
            "10/08/2024 16:17:13 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0']\n",
            "10/08/2024 16:17:13 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0/config.json\n",
            "10/08/2024 16:17:13 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/08/2024 16:17:13 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0/pytorch_model.bin\n",
            "10/08/2024 16:17:18 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   guid: test-10021\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 21404 10726 12426 10221 13132 90998 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   label: contradiction (id = 0)\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   guid: test-10022\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 10338 10380 10221 29789 117 11064 12979 54068 12426 10151 105111 10221 13132 10304 55492 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   guid: test-10023\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 51732 17547 10290 81754 10171 144 106979 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   guid: test-10024\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 43209 10338 10726 10347 91754 117 11064 12979 10726 10128 29991 30325 10329 10268 53709 10338 10151 14001 17033 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   guid: test-10025\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 21023 11569 10140 97611 61139 117 11064 12979 10211 26523 11565 11704 13384 84209 16003 10118 29991 10221 11906 44145 10338 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/08/2024 16:17:19 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/08/2024 16:17:22 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_test_bert-base-multilingual-cased_128_xnli_de\n",
            "10/08/2024 16:17:23 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/08/2024 16:17:23 - INFO - __main__ -     Num examples = 5010\n",
            "10/08/2024 16:17:23 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 627/627 [00:10<00:00, 60.28it/s]\n",
            "10/08/2024 16:17:33 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/08/2024 16:17:33 - INFO - __main__ -     acc = 0.46726546906187627\n"
          ]
        }
      ],
      "source": [
        "!python run_xnli.py --model_type bert --model_name_or_path bert-base-multilingual-cased --language de --train_language de --do_eval --do_train --overwrite_cache --overwrite_output_dir --num_examples 1000 --sampling_strategy \"k_first\" --data_dir \"/content/drive/MyDrive/HighLevel_Tasks/Data/\" --seed 2 --per_gpu_train_batch_size 32 --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 128 --output_dir \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0\" --save_steps -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D-5paVc3_Ev"
      },
      "source": [
        "Phase Two: Continue fine-tuning with English as the source language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HKiknw_3_Ev",
        "outputId": "08b1d526-87f0-4e57-d0df-af1475b73d7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 12272/12272 [37:48<00:00,  5.41it/s]\n",
            "Epoch: 100%|██████████| 2/2 [1:15:35<00:00, 2267.83s/it]\n",
            "10/09/2024 16:18:48 - INFO - __main__ -    global_step = 24544, average loss = 0.4892733137279924\n",
            "10/09/2024 16:18:48 - INFO - __main__ -   Saving model checkpoint to /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0\n",
            "10/09/2024 16:18:48 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0/config.json\n",
            "10/09/2024 16:18:50 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0/pytorch_model.bin\n",
            "10/09/2024 16:18:50 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0/config.json\n",
            "10/09/2024 16:18:50 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/09/2024 16:18:50 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
            "10/09/2024 16:18:55 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/09/2024 16:18:55 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0/vocab.txt\n",
            "10/09/2024 16:18:55 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0/added_tokens.json\n",
            "10/09/2024 16:18:55 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0/special_tokens_map.json\n",
            "10/09/2024 16:18:55 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0/tokenizer_config.json\n",
            "10/09/2024 16:18:55 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0' is a path or url to a directory containing tokenizer files.\n",
            "10/09/2024 16:18:55 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0/vocab.txt\n",
            "10/09/2024 16:18:55 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0/added_tokens.json\n",
            "10/09/2024 16:18:55 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0/special_tokens_map.json\n",
            "10/09/2024 16:18:55 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0/tokenizer_config.json\n",
            "10/09/2024 16:18:55 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0']\n",
            "10/09/2024 16:18:55 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0/config.json\n",
            "10/09/2024 16:18:55 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"xnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "10/09/2024 16:18:55 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0/pytorch_model.bin\n",
            "10/09/2024 16:19:00 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/HighLevel_Tasks/Data/\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   guid: test-10021\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 21404 10726 12426 10221 13132 90998 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   label: contradiction (id = 0)\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   guid: test-10022\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 21023 10338 10380 10221 29789 117 11064 12979 54068 12426 10151 105111 10221 13132 10304 55492 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   guid: test-10023\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   input_ids: 101 34289 117 54122 10143 20530 12979 10726 27955 117 11566 12979 10338 10380 12127 19265 69354 117 11064 12979 10392 12862 21131 10221 13132 30940 10216 119 102 51732 17547 10290 81754 10171 144 106979 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   guid: test-10024\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 43209 10338 10726 10347 91754 117 11064 12979 10726 10128 29991 30325 10329 10268 53709 10338 10151 14001 17033 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   label: neutral (id = 2)\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   guid: test-10025\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   input_ids: 101 1725 41523 12979 10143 20530 117 10242 41750 10290 30797 82065 10240 117 10130 10242 10298 10196 15967 11230 117 10242 10298 52446 10366 10165 11230 119 21023 10338 10118 29991 26216 27746 118 152 117 10134 70273 26523 11565 11704 13384 118 24853 118 53709 10338 119 100 102 21023 11569 10140 97611 61139 117 11064 12979 10211 26523 11565 11704 13384 84209 16003 10118 29991 10221 11906 44145 10338 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/09/2024 16:19:01 - INFO - transformers.data.processors.glue -   label: entailment (id = 1)\n",
            "10/09/2024 16:19:04 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/HighLevel_Tasks/Data/cached_test_xnli_de_k1000_3e-5_2.0_128_xnli_de\n",
            "10/09/2024 16:19:05 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/09/2024 16:19:05 - INFO - __main__ -     Num examples = 5010\n",
            "10/09/2024 16:19:05 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100%|██████████| 627/627 [00:10<00:00, 60.19it/s]\n",
            "10/09/2024 16:19:15 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/09/2024 16:19:15 - INFO - __main__ -     acc = 0.713373253493014\n"
          ]
        }
      ],
      "source": [
        "!python run_xnli.py --model_type bert --model_name_or_path \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_3e-5_2.0\" --language de --train_language en --do_train --do_eval --data_dir \"/content/drive/MyDrive/HighLevel_Tasks/Data/\" --per_gpu_train_batch_size 32 --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 128 --output_dir \"/content/drive/MyDrive/HighLevel_Tasks/Models/xnli_de_k1000_en_3e-5_2.0\" --save_steps -1 2>&1 | tee /content/drive/MyDrive/HighLevel_Tasks/dek1000_en.txt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "lucQRcAIKGQ0"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
